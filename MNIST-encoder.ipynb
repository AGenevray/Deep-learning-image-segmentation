{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes included: 4\n",
      "training set dim(20457, 784).\n",
      "validation set dim(3999, 784).\n",
      "test set dim(4106, 784).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To speed up training we'll only work on a subset of the data containing only the numbers 0, 1, 4, 9.\n",
    "# We discretize the data to 0 and 1 in order to use it with a \n",
    "# bernoulli observation model p(x|z) = Ber(mu(z))\n",
    "\n",
    "def bernoulli_sample(x):\n",
    "    return np.random.binomial(1,x,size=x.shape).astype('float32')\n",
    "\n",
    "# Load data from compressed file with mnist\n",
    "data = np.load('data/mnist.npz')\n",
    "\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.45)\n",
    "\n",
    "# Possible classes\n",
    "classes = list(range(10))\n",
    "\n",
    "# Set the classes we want to use.\n",
    "included_classes = [0, 1, 4, 9] \n",
    "\n",
    "idxs_train = []\n",
    "idxs_valid = []\n",
    "idxs_test = []\n",
    "num_classes = 0\n",
    "for c in included_classes:\n",
    "    if c in classes:\n",
    "        num_classes += 1\n",
    "        idxs_train += np.where(data['y_train'] == c)[0].tolist()\n",
    "        idxs_valid += np.where(data['y_valid'] == c)[0].tolist()\n",
    "        idxs_test += np.where(data['y_test'] == c)[0].tolist()\n",
    "\n",
    "print(\"Number of classes included:\", num_classes)\n",
    "x_train = bernoulli_sample(data['X_train'][idxs_train]).astype('float32')\n",
    "# Since this is unsupervised, the targets are only used for validation.\n",
    "targets_train = data['y_train'][idxs_train].astype('int32')\n",
    "x_train, targets_train = shuffle(x_train, targets_train, random_state=1234)\n",
    "\n",
    "x_valid = bernoulli_sample(data['X_valid'][idxs_valid]).astype('float32')\n",
    "targets_valid = data['y_valid'][idxs_valid].astype('int32')\n",
    "\n",
    "x_test = bernoulli_sample(data['X_test'][idxs_test]).astype('float32')\n",
    "targets_test = data['y_test'][idxs_test].astype('int32')\n",
    "\n",
    "print(\"training set dim(%i, %i).\" % x_train.shape)\n",
    "print(\"validation set dim(%i, %i).\" % x_valid.shape)\n",
    "print(\"test set dim(%i, %i).\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import layers\n",
    "from tensorflow.contrib.layers import fully_connected, convolution2d, convolution2d_transpose, batch_norm, max_pool2d, dropout\n",
    "from tensorflow.python.ops.nn import relu, elu, relu6, sigmoid, tanh, softmax, softplus\n",
    "\n",
    "# Error functions    \n",
    "def sum_of_squared_errors(p, t):\n",
    "    return tf.reduce_sum(tf.square(p - t), axis=[1])\n",
    "\n",
    "\n",
    "# computing cross entropy per sample\n",
    "def categorical_cross_entropy(p, t, eps=1e-10):\n",
    "    return -tf.reduce_sum(t * tf.log(p+eps), axis=[1])\n",
    "\n",
    "\n",
    "def binary_cross_entropy(p, t, eps=1e-10):\n",
    "    return -tf.reduce_sum(t * tf.log(p+eps) + (1-t) * tf.log(1-p+eps), axis=-1)\n",
    "\n",
    "\n",
    "def kl_normal2_stdnormal(mean, log_var, eps=0.0):\n",
    "    return -0.5 * tf.reduce_sum(1 + log_var - tf.square(mean) - tf.exp(log_var), axis=1)\n",
    "\n",
    "\n",
    "c = - 0.5 * math.log(2*math.pi)\n",
    "def log_normal2(x, mean, log_var, eps=0.0):\n",
    "    return tf.reduce_sum(c - log_var/2 - tf.square(x - mean) / (2 * tf.exp(log_var) + eps), axis=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_pl (?, 28, 28, 1)\n",
      "y_pl (?, 28, 28, 4)\n",
      "(?, 28, 28, 16)\n",
      "(?, 14, 14, 64)\n",
      "(?, 7, 7, 144)\n",
      "(?, 7, 7, 384)\n",
      "(?, 14, 14, 528)\n",
      "(?, 28, 28, 416)\n",
      "(?, 28, 28, 4)\n",
      "(?, 28, 28, 4)\n",
      "Model built\n"
     ]
    }
   ],
   "source": [
    "# reset graph\n",
    "reset_default_graph()\n",
    "\n",
    "# -- THE MODEL --#\n",
    "num_channels = 1; #Black and white for MNIST\n",
    "num_classes = len(included_classes)\n",
    "k = 16;\n",
    "height = width = 28\n",
    "\n",
    "# Layer definitions\n",
    "def layer(x, units):\n",
    "    x = fully_connected(x, num_outputs=units, activation_fn=relu,\n",
    "                         normalizer_fn=batch_norm)\n",
    "    x = convolution2d(x, num_outputs=units, kernel_size=(3, 3),\n",
    "                             stride=1)\n",
    "    return dropout(x, is_training=is_training_pl)\n",
    "    \n",
    "def dense_block(x, num_layers):\n",
    "    res = []\n",
    "    for i in range(num_layers):\n",
    "        layer_output = layer(x, k)\n",
    "        x = tf.concat([x, layer_output], axis=-1)\n",
    "        res.append(layer_output)\n",
    "    return x, res\n",
    "    \n",
    "\n",
    "def transition_up(x, units):\n",
    "    return convolution2d_transpose(x, num_outputs=units, kernel_size=(3, 3), stride=2)\n",
    "    \n",
    "    \n",
    "def transition_down(x, units, pooling=True):\n",
    "    #x = batch_norm(x, scope='tdown_batch_norm') #Batch norm should be included in fully_connected layer below\n",
    "    x = fully_connected(x, num_outputs=units, activation_fn=relu,\n",
    "                     normalizer_fn=batch_norm)\n",
    "    x = convolution2d(x, num_outputs=units, kernel_size=(1, 1),\n",
    "                         stride=1)\n",
    "    x = dropout(x, is_training=is_training_pl)\n",
    "    if pooling:\n",
    "        x = max_pool2d(x, kernel_size=(2, 2))\n",
    "    return x\n",
    "\n",
    "# - Tiramisu Architecture - #\n",
    "# Input placeholder\n",
    "x_pl = tf.placeholder(tf.float32, [None, height, width, num_channels], 'x_pl')\n",
    "y_pl = tf.placeholder(tf.float32, [None, height, width, num_classes], 'y_pl')\n",
    "is_training_pl = tf.placeholder(tf.bool, name=\"is-training_pl\")\n",
    "print('x_pl', x_pl.shape)\n",
    "print('y_pl', y_pl.shape)\n",
    "\n",
    "with tf.name_scope('tiramisu'):\n",
    "    # DOWN SAMPLING\n",
    "    x = convolution2d(x_pl, num_outputs=k, kernel_size=(3, 3),\n",
    "                             stride=1, scope=\"pre-convolution\")\n",
    "    print(x.shape)\n",
    "    skip1 = dense_block(x, 4)[0]\n",
    "    \n",
    "    skip1 = tf.concat([x, skip1], axis=-1)\n",
    "    x = transition_down(skip1, 4*16, True)\n",
    "    print(x.shape)\n",
    "\n",
    "    skip2 = dense_block(x, 5)[0]\n",
    "    skip2 = tf.concat([x, skip2], axis=-1)\n",
    "    x = transition_down(skip2, 5*16+4*16, True)\n",
    "    print(x.shape)\n",
    "\n",
    "    # BOTTLENECK\n",
    "    x = dense_block(x, 15)[0]\n",
    "    print(x.shape)\n",
    "\n",
    "    # UPSAMPLING\n",
    "    x = transition_up(x, 15*16)\n",
    "    x = tf.concat([x, skip2], axis=-1)\n",
    "    x = dense_block(x, 5)[0]\n",
    "    print(x.shape)\n",
    "\n",
    "    x = transition_up(x, 5*16)\n",
    "    skipUp = tf.concat([x, skip1], axis=-1)\n",
    "    x = dense_block(skipUp, 4)[0]\n",
    "    x = tf.concat([x, skipUp], axis=-1)\n",
    "    print(x.shape)\n",
    "\n",
    "    # Output layers\n",
    "    x = convolution2d(x, num_outputs=num_classes, kernel_size=(1, 1),\n",
    "                             stride=1, scope=\"post-convolution\")\n",
    "    print(x.shape)\n",
    "    y = fully_connected(x, num_outputs=num_classes, activation_fn=softmax, scope=\"SoftMax\")\n",
    "    print(y.shape)\n",
    "\n",
    "print(\"Model built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(y_pl * tf.log(y+1e-8), reduction_indices=[1])\n",
    "\n",
    "    # averaging over samples\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('training'):\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "    # applying the gradients\n",
    "    train_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('performance'):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pl, axis=1))\n",
    "\n",
    "    # averaging the one-hot encoded vector\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# test the forward pass\n",
    "batch_size = 32\n",
    "x_shape = [batch_size]\n",
    "x_shape.extend(x_pl.get_shape().as_list()[1:])\n",
    "_x_test = np.zeros(shape=x_shape)\n",
    "y_shape = [batch_size]\n",
    "y_shape.extend(y_pl.get_shape().as_list()[1:])\n",
    "_y_test = np.zeros(shape=y_shape)\n",
    "# initialize the Session\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict = {x_pl: _x_test, is_training_pl : True}\n",
    "    y_pred = sess.run(fetches=y, feed_dict=feed_dict)\n",
    "    \n",
    "assert y_pred.shape == _y_test.shape, \"ERROR the output shape is not as expected!\" \\\n",
    "        + \" Output shape should be \" + str(_y_test.shape) + ' but was ' + str(y_pred.shape)\n",
    "\n",
    "print('Forward pass successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
